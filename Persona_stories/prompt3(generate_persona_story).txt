Given the user flows for a specific feature of an app, your task consists of two parts: 
Part1: Create a user persona that is very likely to use the feature described in the user flows in privacy-risky or privacy-invasive ways. The persona should include:
-Basic Persona Info
1.Name, age, and tech comfort level
2.Type of marginalization (select one or more):
(1) Individuals and identities
(2) Physical spaces and communities
(3) Online spaces, tools, and communities
(4) General structural marginalization

-Contextual Persona Info
1. Why do they use the app, and what do they hope to gain from this feature?
2. What privacy expectations, misunderstandings, or vulnerabilities do they bring?
3. What types of information do they care about controlling?

-Privacy Tensions and Costs
** Use the reference section 'Privacy-risky behaviors by marginalized groups' below to identify the persona's privacy behaviors. Address the following:
1. What high-level privacy tensions the user experiences?
2. What specific privacy responses they take or wish they could take
3. What costs do they face when trying to protect their privacy?
-----------------------------------------------------------------------------

Reference: Privacy-risky behaviors by marginalized groups
** Privacy response and cost

1. Apathy
Response: Lack of response to privacy threats
Cost: Exposure to risks
Examples: Undocumented immigrants felt government surveillance is inescapable; women transitioning from incarceration felt they have "nothing to lose"

2. Non-use
Response: Not using technology, deleting accounts
Costs: Opportunity loss, exclusion, silencing, isolation
Example: Economically disadvantaged populations lose opportunities due to non-use of technologies

3. Withholding Disclosure
Response: Self-censorship, information removal  
Costs: Restricts self-expression, silencing
Examples: Low-SES youth, marginalized Cambodians, and political refugees self-censored to avoid conflict but were further silenced; men who have sex with men may not disclose HIV status but inadvertently signal it

4. Controlling Disclosure
Response: Compartmentalizing identity, multiple accounts, privacy controls, segmenting audiences
Costs: Restricts self-expression, labor-intensive, social/financial costs
Examples: Trans men crowdfunding surgery used privacy controls; young Azerbaijanis maintained multiple accounts for activism; LGBTQ+ social media users managed identities across platforms

5. Privacy Lies
Response: Providing false information
Costs: Cognitive burden, social/legal repercussions
Examples: Rural Appalachians provided false information as vigilanteism; South Asian women provided false information to protect from abuse

6. Privacy-Enhancing Technologies
Response: Authentication, cloaking, encryption
Costs: Social liability, erasure of records
Examples: Women in patriarchal societies used private modes and locks which may be seen as incriminating; financially insecure people losing access to two-factor authentication

7. Physical Workarounds
Response: Hiding device, camera covers, headphones
Costs: Limits environmental awareness, vulnerable to physical coercion
Example: People with visual impairments used headphones to avoid aural eavesdropping at cost of physical safety

8. Asking for Help 
Response: Learning practices, consulting network/websites/professionals
Costs: Bad information, involves risk/trust, limited to help available
Examples: IPV professionals not equipped to advise on technology-enabled abuse; visually impaired people risking trust with allies

9. Collaborative Privacy Practices
Response: Shared guidelines, boundaries
Costs: Loss of autonomy, involves risk/trust
Examples: LGBTQ+ adults considering family boundaries; families co-developing privacy guidelines in Bangladesh

10. Third-Party Protections
Response: Parents removing devices, organizations destroying info
Costs: Loss of autonomy, outside control
Examples: Art therapists removing identifying dementia patient information; Canadian government destroying colonial abuse records

**High-Level Privacy Tensions:

1. Privacy vs. Identity Disclosure
Tension between using technology requiring identity disclosure and facing threats
Example: LGBTQ+ social media users connecting while risking stigma from unintended audiences

2. Privacy vs. Suppor
Tension between seeking support and disclosing vulnerabilities 
Examples: Low income people identifying themselves for state benefits; trans men crowdfunding surgery requiring public disclosures

3. Privacy vs. Autonomy
Tension between surveillance for safety and invasion of privacy
Example: VSP shelters surveilling trafficking survivors' communications; location tracking for dementia patients

4. Individual vs. Collective
Tension between individual and group privacy interests
Example: LGBTQ+ parents' disclosures affecting their network's privacy boundaries

-----------------------------------------------------------------------------

Part2: 
Based on the persona and the provided user flows, write a privacy-invasive user journey story with 4–6 scenes.
Each scene should be:
1. Rooted in the user's behavior as described in the persona
2. Aligned with actions found in the given user flows
3. Explicit about the setting, user action, system response, and privacy consequence

INPUT USER FLOWS:

[USER FLOWS]

OUTPUT FORMAT: Return multiple different persona–user journey stories as a JSON object, following this structure: 
{
  "user_persona": "",
  "privacy_invasive user journey": [
    {
      "background": "Brief description of the contextual background for this user journey story. Please be specific—describe the time, situation, and reasons why the persona uses this feature, as well as their goals.",
      "unexpected_outcomes": "Detailed description of disturbing or privacy-invasive outcomes that occur against the user's will. The description should highlight specific harms, including what sensitive data was inappropriately shared, who the data subject is, who disclosed the data (data sender), and who received it (data recipient). The outcome should reflect a subtle or unsettling violation of privacy—something beyond typical or expected consequences.",
      "scenes": [
        {
          "scene_id": "scene_1",
          "scene_description": "A brief description of the user's motive in this scene. Include the reason for transitioning from the previous scene's actions to this one, the user's expectations in this moment, and include direct quotes that reflect the user's thinking or dialogue.",
          "scene_user_flow": [
            {
              "start": "Description of the starting state and context",
              "interface": "Description of what the user sees at the start"
            },
            {
              "step": 1,
              "action": "Describe what the user does in this step",
              "interface": "Describe what the user sees after this action",
              "system_action": "If relevant, describe what the system does behind the scenes"
            },
            // ...additional steps as needed can be added here
            {
              "endpoint": true,
              "interface": "Describe what the user sees at the end of the scene",
              "true_reasoning": "Explain why the user's goal was successfully achieved",
              "next_step": "Describe what the user decides to do next to continue toward their overall goal"
            }
            // OR if the goal was not achieved:
            // {
            //   "endpoint": false,
            //   "interface": "Describe what the user sees at the end of the scene",
            //   "false_reasoning": "Explain why the user's goal was not achieved",
            //   "next_step": "Describe what the user decides to do next to continue toward their overall goal"
            // }
          ]
        }
        // ...additional scenes as needed can be added here
      ]

    }
  ]
}

When describing interfaces in the persona–user journey stories, follow these strict rules:
  1. Use only the terminology, UI elements, and system behaviors explicitly described in the provided input flow.
  2. Use the exact wording from the input flow when referring to interface components (e.g., buttons, screens, options).
  3. Do not invent or assume the existence of any screens, buttons, or features that are not clearly stated in the input.